---
title: "pml_courcera_project"
author: "Konrad Tuszyński"
date: "26.07.2015"
output: html_document
---

## Practical Machine Learning Coursera Project

### Abstract

This is a peer assignment of Coursera's Practical Machine Learning course 2015. The goal
was to build a model able to classify the body actions (standing, running, walking, etc)
out of data provided by smartphone accelerometers. I used randomForest algorithm nad obtained
satisfactory results.

### Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har. 


### Load libraries

```{r}
# load libraries
library(caret)
library(randomForest)
```


### Load datasets

And deletion of columns with missing values.

```{r}
# Import training database
raw.data<- read.csv(file = "pml-training.csv", na.strings=c("","NA"))
# remove colums with NA's
raw.data2<- raw.data[, !(colSums(is.na(raw.data)) >= 1 )]
#load validation dataset (test set for submission)
validation.data<- read.csv(file = "pml-testing.csv", na.strings=c("","NA"))
```

### Predictor selection - deletion of meaningles variables to the model

```{r}
dataset <- raw.data2[, c(-1,-2,-5,-6)]
str(dataset)
```


### Splitting the dataset into the training and testing sets:

```{r}
inTrain = createDataPartition(dataset$classe, p = 0.1, list = F)
training = dataset[ inTrain,]
testing = dataset[-inTrain,]
```

The training set consisted of `r nrow(training)` rows.
The testing set consisted of `r nrow(testing)` rows.

The validation set (for submission) consisted of `r nrow(validation.data)` rows.

*This inapproriate split ratio is due to the fact, that old IBM tablet was used and training 
took 2hrs even on 10% of records... But still, prediction error is suprisingly low.*

### Model training

Actual model training:

```{r}
#modFit <- train(classe ~ ., data=training, method="rf")  # hashed because knitted from saved model
#save(modFit, file="modFit1.RData")
```

randomForest was choosen as a modeling tool due to its good clasification capabilities.
It also does cross-validation automatically.

Loading previously built model:

```{r}
load("modFit1.RData")
#oad("modFit2.RData")  #second run
```


### Model details:

```{r}
print(modFit)
```



```{r}
print(modFit$finalModel)
```

ConfusionMatrix for traininset (internal validation)
```{r}
train.pred <- predict(modFit, training)
confusionMatrix(training$classe, train.pred)
confmx1 <-confusionMatrix(training$classe, train.pred)
accuracy1 <- round(confmx1$overall[[1]]*100, 1)
```

### Input variables relative importance is shown on the plot:

```{r}
plot(varImp(modFit, ))
```


### Validation with testing dataset

confusionMatrix for testing dataset:

```{r}
test.pred <- predict(modFit, testing)
confusionMatrix(testing$classe, test.pred)
confmx2 <-confusionMatrix(testing$classe, test.pred)
accuracy2 <- round(confmx2$overall[[1]]*100, 1)
```

Accuracy has increased from `r accuracy1`% for training set to `r accuracy2`% for 
validation test. It means that in this example out-of-sample error is even lower than this obtained
on training dataset. Suprisingly, considering that only 10% of dataset was used for training.


### Validation online (test set of 20 records)

Model was validated via online submission:

```{r}
val.pred <- predict(modFit, validation.data)
val.pred
```

20/20 correct



